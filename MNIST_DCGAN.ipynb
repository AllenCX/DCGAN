{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys \n",
    "import os\n",
    "from config import Config\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "    def __init__(self, input, sess, config):\n",
    "        self.input = input\n",
    "        self.X, self.y = self.load_mnist_data()\n",
    "        self.config = config\n",
    "        self.sess = sess\n",
    "        self.idx = 0\n",
    "        self.iters = 0\n",
    "        #print(self.X.shape, self.y.shape)\n",
    "    def _leakyRelu(self, input, alpha, name='leakyRelu'):\n",
    "        return tf.maximum(input, alpha * input, name=name)\n",
    "        \n",
    "    def _D_conv2d_bn(self, input, output_num, filter_size, stride, is_training=False, name=None):\n",
    "        with tf.variable_scope(name) as scope:\n",
    "            conv = tf.contrib.layers.conv2d(input, output_num, filter_size, stride, \n",
    "                                            'SAME', activation_fn=None, biases_initializer=None) \n",
    "            bn = tf.contrib.layers.batch_norm(inputs=conv, is_training=is_training, activation_fn=None)\n",
    "            return self._leakyRelu(bn, self.config.alpha)\n",
    "        \n",
    "    def _G_conv_transpose_bn(self, input, output_num, filter_size, stride, is_training, name=None):\n",
    "        with tf.variable_scope(name) as scope:\n",
    "            conv_transpose = tf.contrib.layers.conv2d_transpose(\n",
    "                input, output_num, filter_size, stride, 'SAME', \n",
    "                activation_fn=None, biases_initializer=None, scope=name)\n",
    "            bn = tf.contrib.layers.batch_norm(inputs=conv_transpose,\n",
    "                                              is_training=is_training, \n",
    "                                              activation_fn=None, \n",
    "                                              scope=name)\n",
    "            return tf.nn.relu(bn)\n",
    "        \n",
    "    def discriminator(self, input, y=None, reuse=False, is_training=False):\n",
    "        # LeNet-like discriminator\n",
    "        with tf.variable_scope(\"discriminator\") as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "            d_1 = self._D_conv2d_bn(input, 8, 3, 1, is_training, name=\"D_conv1\") # => [None, 14, 14, 8]\n",
    "            d_2 = self._D_conv2d_bn(d_1, 16, 3, 2, is_training, name=\"D_conv2\")  # => [None, 7, 7, 16]\n",
    "            d_3 = self._D_conv2d_bn(d_2, 32, 3, 2, is_training, name=\"D_conv3\")  # => [None, 7, 7, 16]\n",
    "            d_4 = self._D_conv2d_bn(d_3, 64, 3, 1, is_training, name=\"D_conv4\")  # => [None, 7, 7, 16]\n",
    "            d_5 = tf.contrib.layers.flatten(d_4)\n",
    "            d_6 = tf.contrib.layers.fully_connected(d_5, num_outputs=1, activation_fn=None, scope=\"D_linear\")\n",
    "            return tf.nn.sigmoid(d_6, name=\"D_output\"), d_6\n",
    "        \n",
    "    def generator(self, input, reuse=False, is_training=False):\n",
    "        with tf.variable_scope(\"generator\") as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variable()\n",
    "            g_0 = tf.cast(\n",
    "                tf.contrib.layers.fully_connected(input, 7 * 7 * 128, \n",
    "                                                  activation_fn=None, \n",
    "                                                  scope=\"G_linear\"), \n",
    "                dtype=tf.float32)\n",
    "            # => [None, 4 * 4 * 1024]\n",
    "            g_1 = tf.cast(tf.reshape(g_0, shape=[-1, 7, 7, 128]), dtype=tf.float32)\n",
    "            g_2 = self._G_conv_transpose_bn(g_1, 64, 3, 2, is_training=is_training, name=\"G_conv_trans_1\")\n",
    "            g_3 = self._G_conv_transpose_bn(g_2, 32, 3, 1, is_training=is_training, name=\"G_conv_trans_2\")\n",
    "            g_4 = self._G_conv_transpose_bn(g_3, 16, 3, 2, is_training=is_training, name=\"G_conv_trans_3\")\n",
    "            g_5 = self._G_conv_transpose_bn(g_4, 1, 3, 1, is_training=is_training, name=\"G_conv_trans_4\")\n",
    "            return tf.nn.tanh(g_5)\n",
    "            \n",
    "    def build(self, config):\n",
    "        print(\"Building model...\")\n",
    "        self.is_training = tf.placeholder(bool, name=\"is_training\")\n",
    "        self.noise = tf.placeholder(shape=(None, config.latent_size),\n",
    "                                    dtype=tf.float32, name=\"noise\")\n",
    "        self.real_image = tf.placeholder(shape=(config.batch_size, 28, 28, 1), \n",
    "                                         dtype=tf.float32, name=\"real_image\")\n",
    "        \n",
    "        self._D_real, self.real_logits = self.discriminator(self.real_image,\n",
    "                                                            y=None, \n",
    "                                                            is_training=self.is_training, \n",
    "                                                            reuse=False)\n",
    "        self.fake_image = self.generator(self.noise, is_training=self.is_training)\n",
    "        \n",
    "        self._D_fake, self.fake_logits = self.discriminator(self.fake_image, \n",
    "                                                            y=None,\n",
    "                                                            is_training=self.is_training, \n",
    "                                                            reuse=True)\n",
    "        \n",
    "        self.G_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_logits, \n",
    "                                                    labels=tf.ones_like(self._D_fake)))\n",
    "        self.D_loss_fake = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_logits, \n",
    "                                                    labels=tf.zeros_like(self._D_fake)))\n",
    "        \n",
    "        self.D_loss_real = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.real_logits, \n",
    "                                                    labels=tf.ones_like(self._D_real)))\n",
    "        self.D_loss = self.D_loss_real + self.D_loss_fake\n",
    "        \n",
    "        self.G_image_sum = tf.summary.image(\"G_fake_image\", self.fake_image)\n",
    "        self.G_loss_sum = tf.summary.scalar(\"G_loss\", self.G_loss)\n",
    "        \n",
    "        self.D_loss_sum = tf.summary.scalar(\"D_loss\", self.D_loss)\n",
    "        self.D_real_loss_sum = tf.summary.scalar(\"D_loss_real\", self.D_loss_real)\n",
    "        self.D_fake_loss_sum = tf.summary.scalar(\"D_loss_fake\", self.D_loss_fake)\n",
    "        \n",
    "        self.G_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=\"generator\") \n",
    "        self.D_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=\"discriminator\")\n",
    "        self.updata_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        \n",
    "        self.G_trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"generator\")\n",
    "        self.D_trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"discriminator\")\n",
    "        self.trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        \n",
    "        self.G_sum = tf.summary.merge([self.G_image_sum, self.G_loss_sum])\n",
    "        self.D_sum = tf.summary.merge([self.D_loss_sum, self.D_real_loss_sum, self.D_fake_loss_sum])\n",
    "        \n",
    "        self.train_summary_writer = tf.summary.FileWriter(config.summary_dir + \"/train\", self.sess.graph)\n",
    "        self.test_summary_writer = tf.summary.FileWriter(config.summary_dir + \"/test\")\n",
    "        \n",
    "    def train(self, config):\n",
    "        self.build(config)\n",
    "        \n",
    "        sess = self.sess\n",
    "        print(\"Starting training...\")\n",
    "        self.D_global_step = tf.Variable(0)\n",
    "        self.G_global_step = tf.Variable(0)\n",
    "        with tf.control_dependencies(self.D_update_ops):\n",
    "            D_opt = tf.train.AdamOptimizer(learning_rate=config.lr, beta1=config.beta1) \\\n",
    "                            .minimize(self.D_loss, var_list=self.D_trainable_vars, global_step=self.D_global_step)\n",
    "        with tf.control_dependencies(self.G_update_ops):\n",
    "            G_opt = tf.train.AdamOptimizer(learning_rate=config.lr, beta1=config.beta1) \\\n",
    "                            .minimize(self.G_loss, var_list=self.G_trainable_vars, global_step=self.G_global_step)\n",
    "        \n",
    "        noise = np.random.uniform(-1, 1, [config.batch_size, config.latent_size])\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(config.epoch_num):\n",
    "            print(\"Epoch %d\" % (epoch+1))\n",
    "            batch_ix = 0\n",
    "            for X_batch, y_batch in self.minibatches(self.X, self.y, config.batch_size, True):\n",
    "                feed_dict = {\n",
    "                    self.real_image: X_batch.astype(np.float32), \n",
    "                    self.noise: noise,\n",
    "                    self.is_training: True\n",
    "                }\n",
    "                \n",
    "                _, train_D_loss, D_global_step, sum_str = sess.run([D_opt, self.D_loss, self.D_global_step, self.D_sum], \n",
    "                                                          feed_dict=feed_dict)\n",
    "                self.train_summary_writer.add_summary(sum_str, D_global_step)\n",
    "                \n",
    "                _, train_G_loss, G_global_step, sum_str = sess.run([G_opt, self.G_loss, self.G_global_step, self.G_sum], \n",
    "                                                          feed_dict=feed_dict)\n",
    "                self.train_summary_writer.add_summary(sum_str, G_global_step)\n",
    "                \n",
    "                _, train_G_loss, G_global_step, sum_str = sess.run([G_opt, self.G_loss, self.G_global_step, self.G_sum], \n",
    "                                                          feed_dict=feed_dict)\n",
    "                self.train_summary_writer.add_summary(sum_str, G_global_step)\n",
    "                batch_ix += 1\n",
    "                sys.stdout.write(\"%d / %d batches have been trained \\r\" % (batch_ix, 70000 // config.batch_size))\n",
    "                sys.stdout.flush()\n",
    "                # Just test\n",
    "                '''if D_global_step % 50 == 0:\n",
    "                    print(D_global_step, train_D_loss, train_G_loss)\n",
    "                if D_global_step > 5001:\n",
    "                    real_score, fake_score = sess.run([self._D_real, self._D_fake], \n",
    "                                      feed_dict={\n",
    "                                          self.noise: noise,\n",
    "                                          self.real_image:X_batch,\n",
    "                                          self.is_training: False\n",
    "                                      })\n",
    "                    print(real_score[:10])\n",
    "                    print(fake_score[:10])\n",
    "                    print('=' * 80)\n",
    "                    preview = sess.run(self.fake_image, \n",
    "                                       feed_dict={\n",
    "                                           self.noise: noise,\n",
    "                                           self.is_training: False\n",
    "                                       })\n",
    "                    return preview'''\n",
    "        print(\"Training finished!\")    \n",
    "    def generate(self, num):\n",
    "        sess = self.sess\n",
    "        noise = np.random.uniform(-1, 1, [num, config.latent_size])\n",
    "        fake_image = sess.run(self.fake_image,\n",
    "                             feed_dict={\n",
    "                                 self.noise: noise,\n",
    "                                 self.is_training: False\n",
    "                             })\n",
    "        return fake_image.reshape(num, 28, 28)\n",
    "    def load_mnist_data(self, path=\"mnist.npz\"):\n",
    "        data = np.load(path)\n",
    "        X = np.concatenate((data[\"x_train\"], data[\"x_test\"]), axis=0) / 255.0\n",
    "        y = np.concatenate((data[\"y_train\"], data[\"y_test\"]), axis=0) / 255.0\n",
    "        X = np.expand_dims(X, axis=3)\n",
    "        return X, y\n",
    "    \n",
    "    def minibatches(self, inputs=None, targets=None, batch_size=None, shuffle=False):\n",
    "        assert len(inputs) == len(targets)\n",
    "        if shuffle:\n",
    "            indices = np.arange(len(inputs))\n",
    "            np.random.shuffle(indices)\n",
    "        for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
    "            if shuffle:\n",
    "                excerpt = indices[start_idx:start_idx + batch_size]\n",
    "            else:\n",
    "                excerpt = slice(start_idx, start_idx + batch_size)\n",
    "            yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Starting training...\n",
      "Epoch 1\n",
      "150 / 1093 batches have been trained \r"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "config = Config(batch_size=64, latent_size=100, lr=0.0002, epoch_num=1, beta1=0.5, alpha=0.2)\n",
    "test_input = tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.float32)\n",
    "test_model = DCGAN(test_input, tf.Session(), config)\n",
    "test_model.train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview = test_model.generate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16504630630>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nl5oyfqkp45eaMn6pKeOXmvp/ZAVWvVDGB7EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x165045968d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preview[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preview[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
