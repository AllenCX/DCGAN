{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys \n",
    "import os\n",
    "from config import Config\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "    def __init__(self, input, sess, config):\n",
    "        self.input = input\n",
    "        self.X, self.y = self.load_mnist_data()\n",
    "        self.config = config\n",
    "        self.sess = sess\n",
    "        self.idx = 0\n",
    "        self.iters = 0\n",
    "        #print(self.X.shape, self.y.shape)\n",
    "    def _leakyRelu(self, input, alpha, name='leakyRelu'):\n",
    "        return tf.maximum(input, alpha * input, name=name)\n",
    "        \n",
    "    def _D_conv2d_bn(self, input, output_num, filter_size, stride, is_training=False, name=None):\n",
    "        with tf.variable_scope(name) as scope:\n",
    "            conv = tf.contrib.layers.conv2d(input, output_num, filter_size, stride, \n",
    "                                            'SAME', activation_fn=None, biases_initializer=None) \n",
    "            bn = tf.contrib.layers.batch_norm(inputs=conv, is_training=is_training, activation_fn=None)\n",
    "            return self._leakyRelu(bn, self.config.alpha)\n",
    "        \n",
    "    def _G_conv_transpose_bn(self, input, output_num, filter_size, stride, is_training, name=None):\n",
    "        with tf.variable_scope(name) as scope:\n",
    "            conv_transpose = tf.contrib.layers.conv2d_transpose(\n",
    "                input, output_num, filter_size, stride, 'SAME', \n",
    "                activation_fn=None, biases_initializer=None, scope=name)\n",
    "            bn = tf.contrib.layers.batch_norm(inputs=conv_transpose,\n",
    "                                              is_training=is_training, \n",
    "                                              activation_fn=None, \n",
    "                                              scope=name)\n",
    "            return tf.nn.relu(bn)\n",
    "        \n",
    "    def discriminator(self, input, y=None, reuse=False, is_training=False):\n",
    "        # LeNet-like discriminator\n",
    "        with tf.variable_scope(\"discriminator\") as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "            d_1 = self._D_conv2d_bn(input, 8, 3, 1, is_training, name=\"D_conv1\") # => [None, 14, 14, 8]\n",
    "            d_2 = self._D_conv2d_bn(d_1, 16, 3, 2, is_training, name=\"D_conv2\")  # => [None, 7, 7, 16]\n",
    "            d_3 = self._D_conv2d_bn(d_2, 32, 3, 2, is_training, name=\"D_conv3\")  # => [None, 7, 7, 16]\n",
    "            d_4 = self._D_conv2d_bn(d_3, 64, 3, 1, is_training, name=\"D_conv4\")  # => [None, 7, 7, 16]\n",
    "            d_5 = tf.contrib.layers.flatten(d_4)\n",
    "            d_6 = tf.contrib.layers.fully_connected(d_5, num_outputs=1, activation_fn=None, scope=\"D_linear\")\n",
    "            return tf.nn.sigmoid(d_6, name=\"D_output\"), d_6\n",
    "        \n",
    "    def generator(self, input, reuse=False, is_training=False):\n",
    "        with tf.variable_scope(\"generator\") as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variable()\n",
    "            g_0 = tf.cast(\n",
    "                tf.contrib.layers.fully_connected(input, 7 * 7 * 128, \n",
    "                                                  activation_fn=None, \n",
    "                                                  scope=\"G_linear\"), \n",
    "                dtype=tf.float32)\n",
    "            # => [None, 4 * 4 * 1024]\n",
    "            g_1 = tf.cast(tf.reshape(g_0, shape=[-1, 7, 7, 128]), dtype=tf.float32)\n",
    "            g_2 = self._G_conv_transpose_bn(g_1, 64, 3, 2, is_training=is_training, name=\"G_conv_trans_1\")\n",
    "            g_3 = self._G_conv_transpose_bn(g_2, 32, 3, 1, is_training=is_training, name=\"G_conv_trans_2\")\n",
    "            g_4 = self._G_conv_transpose_bn(g_3, 16, 3, 2, is_training=is_training, name=\"G_conv_trans_3\")\n",
    "            g_5 = self._G_conv_transpose_bn(g_4, 1, 3, 1, is_training=is_training, name=\"G_conv_trans_4\")\n",
    "            return tf.nn.tanh(g_5)\n",
    "            \n",
    "    def build(self, config):\n",
    "        print(\"Building model...\")\n",
    "        self.is_training = tf.placeholder(bool, name=\"is_training\")\n",
    "        self.noise = tf.placeholder(shape=(None, config.latent_size),\n",
    "                                    dtype=tf.float32, name=\"noise\")\n",
    "        self.real_image = tf.placeholder(shape=(config.batch_size, 28, 28, 1), \n",
    "                                         dtype=tf.float32, name=\"real_image\")\n",
    "        \n",
    "        self._D_real, self.real_logits = self.discriminator(self.real_image,\n",
    "                                                            y=None, \n",
    "                                                            is_training=self.is_training, \n",
    "                                                            reuse=False)\n",
    "        self.fake_image = self.generator(self.noise, is_training=self.is_training)\n",
    "        \n",
    "        self._D_fake, self.fake_logits = self.discriminator(self.fake_image, \n",
    "                                                            y=None,\n",
    "                                                            is_training=self.is_training, \n",
    "                                                            reuse=True)\n",
    "        \n",
    "        self.G_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_logits, \n",
    "                                                    labels=tf.ones_like(self._D_fake)))\n",
    "        self.D_loss_fake = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_logits, \n",
    "                                                    labels=tf.zeros_like(self._D_fake)))\n",
    "        \n",
    "        self.D_loss_real = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.real_logits, \n",
    "                                                    labels=tf.ones_like(self._D_real)))\n",
    "        self.D_loss = self.D_loss_real + self.D_loss_fake\n",
    "        \n",
    "        self.G_image_sum = tf.summary.image(\"G_fake_image\", self.fake_image)\n",
    "        self.G_loss_sum = tf.summary.scalar(\"G_loss\", self.G_loss)\n",
    "        \n",
    "        self.D_loss_sum = tf.summary.scalar(\"D_loss\", self.D_loss)\n",
    "        self.D_real_loss_sum = tf.summary.scalar(\"D_loss_real\", self.D_loss_real)\n",
    "        self.D_fake_loss_sum = tf.summary.scalar(\"D_loss_fake\", self.D_loss_fake)\n",
    "        \n",
    "        self.G_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=\"generator\") \n",
    "        self.D_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=\"discriminator\")\n",
    "        self.updata_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        \n",
    "        self.G_trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"generator\")\n",
    "        self.D_trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"discriminator\")\n",
    "        self.trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        \n",
    "        self.G_sum = tf.summary.merge([self.G_image_sum, self.G_loss_sum])\n",
    "        self.D_sum = tf.summary.merge([self.D_loss_sum, self.D_real_loss_sum, self.D_fake_loss_sum])\n",
    "        \n",
    "        self.train_summary_writer = tf.summary.FileWriter(config.summary_dir + \"/train\", self.sess.graph)\n",
    "        self.test_summary_writer = tf.summary.FileWriter(config.summary_dir + \"/test\")\n",
    "        \n",
    "    def train(self, config):\n",
    "        self.build(config)\n",
    "        \n",
    "        #sess = self.sess\n",
    "        print(\"Starting training...\")\n",
    "        self.D_global_step = tf.Variable(0)\n",
    "        self.G_global_step = tf.Variable(0)\n",
    "        with tf.control_dependencies(self.D_update_ops):\n",
    "            D_opt = tf.train.AdamOptimizer(learning_rate=config.lr, beta1=config.beta1) \\\n",
    "                            .minimize(self.D_loss, var_list=self.D_trainable_vars, global_step=self.D_global_step)\n",
    "        with tf.control_dependencies(self.G_update_ops):\n",
    "            G_opt = tf.train.AdamOptimizer(learning_rate=config.lr, beta1=config.beta1) \\\n",
    "                            .minimize(self.G_loss, var_list=self.G_trainable_vars, global_step=self.G_global_step)\n",
    "        \n",
    "        noise = np.random.uniform(-1, 1, [config.batch_size, config.latent_size])\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(config.epoch_num):\n",
    "            print(\"Epoch %d\" % (epoch+1))\n",
    "            batch_ix = 0\n",
    "            for X_batch, y_batch in self.minibatches(self.X, self.y, config.batch_size, True):\n",
    "                feed_dict = {\n",
    "                    self.real_image: X_batch.astype(np.float32), \n",
    "                    self.noise: noise,\n",
    "                    self.is_training: True\n",
    "                }\n",
    "                \n",
    "                _, train_D_loss, D_global_step, sum_str = self.sess.run(\n",
    "                    [D_opt, self.D_loss, self.D_global_step, self.D_sum], feed_dict=feed_dict)\n",
    "                self.train_summary_writer.add_summary(sum_str, D_global_step)\n",
    "                \n",
    "                _, train_G_loss, G_global_step, sum_str = self.sess.run(\n",
    "                    [G_opt, self.G_loss, self.G_global_step, self.G_sum], feed_dict=feed_dict)\n",
    "                self.train_summary_writer.add_summary(sum_str, G_global_step)\n",
    "                \n",
    "                _, train_G_loss, G_global_step, sum_str = self.sess.run(\n",
    "                    [G_opt, self.G_loss, self.G_global_step, self.G_sum], feed_dict=feed_dict)\n",
    "                self.train_summary_writer.add_summary(sum_str, G_global_step)\n",
    "                batch_ix += 1\n",
    "                sys.stdout.write(\"%d / %d batches have been trained \\r\" % (batch_ix, 70000 // config.batch_size))\n",
    "                sys.stdout.flush()\n",
    "                # Just test\n",
    "                '''if D_global_step % 50 == 0:\n",
    "                    print(D_global_step, train_D_loss, train_G_loss)\n",
    "                if D_global_step > 5001:\n",
    "                    real_score, fake_score = self.sess.run([self._D_real, self._D_fake], \n",
    "                                      feed_dict={\n",
    "                                          self.noise: noise,\n",
    "                                          self.real_image:X_batch,\n",
    "                                          self.is_training: False\n",
    "                                      })\n",
    "                    print(real_score[:10])\n",
    "                    print(fake_score[:10])\n",
    "                    print('=' * 80)\n",
    "                    preview = self.sess.run(self.fake_image, \n",
    "                                       feed_dict={\n",
    "                                           self.noise: noise,\n",
    "                                           self.is_training: False\n",
    "                                       })\n",
    "                    return preview'''\n",
    "        print(\"Training finished!\")    \n",
    "    def generate(self, num):\n",
    "        #sess = self.sess\n",
    "        noise = np.random.uniform(-1, 1, [num, config.latent_size])\n",
    "        fake_image = self.sess.run(self.fake_image,\n",
    "                             feed_dict={\n",
    "                                 self.noise: noise,\n",
    "                                 self.is_training: False\n",
    "                             })\n",
    "        return fake_image.reshape(num, 28, 28)\n",
    "    def load_mnist_data(self, path=\"mnist.npz\"):\n",
    "        data = np.load(path)\n",
    "        X = np.concatenate((data[\"x_train\"], data[\"x_test\"]), axis=0) / 255.0\n",
    "        y = np.concatenate((data[\"y_train\"], data[\"y_test\"]), axis=0) / 255.0\n",
    "        X = np.expand_dims(X, axis=3)\n",
    "        return X, y\n",
    "    \n",
    "    def minibatches(self, inputs=None, targets=None, batch_size=None, shuffle=False):\n",
    "        assert len(inputs) == len(targets)\n",
    "        if shuffle:\n",
    "            indices = np.arange(len(inputs))\n",
    "            np.random.shuffle(indices)\n",
    "        for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
    "            if shuffle:\n",
    "                excerpt = indices[start_idx:start_idx + batch_size]\n",
    "            else:\n",
    "                excerpt = slice(start_idx, start_idx + batch_size)\n",
    "            yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "config = Config(batch_size=64, latent_size=100, lr=0.0002, epoch_num=1, beta1=0.5, alpha=0.2)\n",
    "test_input = tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.float32)\n",
    "test_model = DCGAN(test_input, tf.Session(), config)\n",
    "test_model.train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview = test_model.generate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(preview[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
