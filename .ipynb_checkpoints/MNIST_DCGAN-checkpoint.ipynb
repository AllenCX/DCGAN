{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys \n",
    "import os\n",
    "from config import Config\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "    def __init__(self, input, sess, config):\n",
    "        self.input = input\n",
    "        self.X, self.y = self.load_mnist_data()\n",
    "        self.config = config\n",
    "        self.sess = sess\n",
    "        self.idx = 0\n",
    "        self.iters = 0\n",
    "        #print(self.X.shape, self.y.shape)\n",
    "    def _leakyRelu(self, input, alpha, name='leakyRelu'):\n",
    "        return tf.maximum(input, alpha * input, name=name)\n",
    "        \n",
    "    def _D_conv2d_bn(self, input, output_num, filter_size, stride, is_training=False, name=None):\n",
    "        with tf.variable_scope(name) as scope:\n",
    "            conv = tf.contrib.layers.conv2d(input, output_num, filter_size, stride, \n",
    "                                            'SAME', activation_fn=None, biases_initializer=None) \n",
    "            bn = tf.contrib.layers.batch_norm(inputs=conv, is_training=is_training, activation_fn=None)\n",
    "            return self._leakyRelu(bn, self.config.alpha)\n",
    "        \n",
    "    def _G_conv_transpose_bn(self, input, output_num, filter_size, stride, is_training, name=None):\n",
    "        with tf.variable_scope(name) as scope:\n",
    "            conv_transpose = tf.contrib.layers.conv2d_transpose(\n",
    "                input, output_num, filter_size, stride, 'SAME', \n",
    "                activation_fn=None, biases_initializer=None, scope=name)\n",
    "            bn = tf.contrib.layers.batch_norm(inputs=conv_transpose,\n",
    "                                              is_training=is_training, \n",
    "                                              activation_fn=None, \n",
    "                                              scope=name)\n",
    "            return tf.nn.relu(bn)\n",
    "        \n",
    "    def discriminator(self, input, y=None, reuse=False, is_training=False):\n",
    "        # LeNet-like discriminator\n",
    "        with tf.variable_scope(\"discriminator\") as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "            d_1 = self._D_conv2d_bn(input, 8, 3, 1, is_training, name=\"D_conv1\") # => [None, 14, 14, 8]\n",
    "            d_2 = self._D_conv2d_bn(d_1, 16, 3, 2, is_training, name=\"D_conv2\")  # => [None, 7, 7, 16]\n",
    "            d_3 = self._D_conv2d_bn(d_2, 32, 3, 2, is_training, name=\"D_conv3\")  # => [None, 7, 7, 16]\n",
    "            d_4 = self._D_conv2d_bn(d_3, 64, 3, 1, is_training, name=\"D_conv4\")  # => [None, 7, 7, 16]\n",
    "            d_5 = tf.contrib.layers.flatten(d_4)\n",
    "            d_6 = tf.contrib.layers.fully_connected(d_5, num_outputs=1, activation_fn=None, scope=\"D_linear\")\n",
    "            return tf.nn.sigmoid(d_6, name=\"D_output\"), d_6\n",
    "        \n",
    "    def generator(self, input, reuse=False, is_training=False):\n",
    "        with tf.variable_scope(\"generator\") as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variable()\n",
    "            g_0 = tf.cast(\n",
    "                tf.contrib.layers.fully_connected(input, 7 * 7 * 128, \n",
    "                                                  activation_fn=None, \n",
    "                                                  scope=\"G_linear\"), \n",
    "                dtype=tf.float32)\n",
    "            # => [None, 4 * 4 * 1024]\n",
    "            g_1 = tf.cast(tf.reshape(g_0, shape=[-1, 7, 7, 128]), dtype=tf.float32)\n",
    "            g_2 = self._G_conv_transpose_bn(g_1, 64, 3, 2, is_training=is_training, name=\"G_conv_trans_1\")\n",
    "            g_3 = self._G_conv_transpose_bn(g_2, 32, 3, 1, is_training=is_training, name=\"G_conv_trans_2\")\n",
    "            g_4 = self._G_conv_transpose_bn(g_3, 16, 3, 2, is_training=is_training, name=\"G_conv_trans_3\")\n",
    "            g_5 = self._G_conv_transpose_bn(g_4, 1, 3, 1, is_training=is_training, name=\"G_conv_trans_4\")\n",
    "            return tf.nn.tanh(g_5)\n",
    "            \n",
    "    def build(self, config):\n",
    "        print(\"Building model...\")\n",
    "        self.is_training = tf.placeholder(bool, name=\"is_training\")\n",
    "        self.noise = tf.placeholder(shape=(config.batch_size, config.latent_size),\n",
    "                                    dtype=tf.float32, name=\"noise\")\n",
    "        self.real_image = tf.placeholder(shape=(config.batch_size, 28, 28, 1), \n",
    "                                         dtype=tf.float32, name=\"real_image\")\n",
    "        \n",
    "        self._D_real, self.real_logits = self.discriminator(self.real_image,\n",
    "                                                            y=None, \n",
    "                                                            is_training=self.is_training, \n",
    "                                                            reuse=False)\n",
    "        self.fake_image = self.generator(self.noise, is_training=self.is_training)\n",
    "        \n",
    "        self._D_fake, self.fake_logits = self.discriminator(self.fake_image, \n",
    "                                                            y=None,\n",
    "                                                            is_training=self.is_training, \n",
    "                                                            reuse=True)\n",
    "        \n",
    "        self.G_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_logits, \n",
    "                                                    labels=tf.ones_like(self._D_fake)))\n",
    "        self.D_loss_fake = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_logits, \n",
    "                                                    labels=tf.zeros_like(self._D_fake)))\n",
    "        \n",
    "        self.D_loss_real = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.real_logits, \n",
    "                                                    labels=tf.ones_like(self._D_real)))\n",
    "        self.D_loss = self.D_loss_real + self.D_loss_fake\n",
    "        \n",
    "        self.G_image_sum = tf.summary.image(\"G_fake_image\", self.fake_image)\n",
    "        self.G_loss_sum = tf.summary.scalar(\"G_loss\", self.G_loss)\n",
    "        \n",
    "        self.D_loss_sum = tf.summary.scalar(\"D_loss\", self.D_loss)\n",
    "        self.D_real_loss_sum = tf.summary.scalar(\"D_loss_real\", self.D_loss_real)\n",
    "        self.D_fake_loss_sum = tf.summary.scalar(\"D_loss_fake\", self.D_loss_fake)\n",
    "        \n",
    "        self.G_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=\"generator\") \n",
    "        self.D_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=\"discriminator\")\n",
    "        self.updata_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        \n",
    "        self.G_trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"generator\")\n",
    "        self.D_trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"discriminator\")\n",
    "        self.trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        \n",
    "        self.G_sum = tf.summary.merge([self.G_image_sum, self.G_loss_sum])\n",
    "        self.D_sum = tf.summary.merge([self.D_loss_sum, self.D_real_loss_sum, self.D_fake_loss_sum])\n",
    "        \n",
    "        self.train_summary_writer = tf.summary.FileWriter(config.summary_dir + \"/train\", self.sess.graph)\n",
    "        self.test_summary_writer = tf.summary.FileWriter(config.summary_dir + \"/test\")\n",
    "        \n",
    "    def train(self, config):\n",
    "        self.build(config)\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        self.D_global_step = tf.Variable(0)\n",
    "        self.G_global_step = tf.Variable(0)\n",
    "        with tf.control_dependencies(self.D_update_ops):\n",
    "            D_opt = tf.train.AdamOptimizer(learning_rate=config.lr, beta1=config.beta1) \\\n",
    "                            .minimize(self.D_loss, var_list=self.D_trainable_vars, global_step=self.D_global_step)\n",
    "        with tf.control_dependencies(self.G_update_ops):\n",
    "            G_opt = tf.train.AdamOptimizer(learning_rate=config.lr, beta1=config.beta1) \\\n",
    "                            .minimize(self.G_loss, var_list=self.G_trainable_vars, global_step=self.G_global_step)\n",
    "        \n",
    "        noise = np.random.uniform(-1, 1, [config.batch_size, config.latent_size])\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(config.epoch_num):\n",
    "            print(\"Epoch %d\" % (epoch+1))\n",
    "            batch_ix = 0\n",
    "            for X_batch, y_batch in self.minibatches(self.X, self.y, config.batch_size, True):\n",
    "                feed_dict = {\n",
    "                    self.real_image: X_batch.astype(np.float32), \n",
    "                    self.noise: noise,\n",
    "                    self.is_training: True\n",
    "                }\n",
    "                \n",
    "                _, train_D_loss, D_global_step, sum_str = sess.run([D_opt, self.D_loss, self.D_global_step, self.D_sum], \n",
    "                                                          feed_dict=feed_dict)\n",
    "                self.train_summary_writer.add_summary(sum_str, D_global_step)\n",
    "                \n",
    "                _, train_G_loss, G_global_step, sum_str = sess.run([G_opt, self.G_loss, self.G_global_step, self.G_sum], \n",
    "                                                          feed_dict=feed_dict)\n",
    "                self.train_summary_writer.add_summary(sum_str, G_global_step)\n",
    "                \n",
    "                _, train_G_loss, G_global_step, sum_str = sess.run([G_opt, self.G_loss, self.G_global_step, self.G_sum], \n",
    "                                                          feed_dict=feed_dict)\n",
    "                self.train_summary_writer.add_summary(sum_str, G_global_step)\n",
    "                batch_ix += 1\n",
    "                sys.stdout.write(\"%d / %d samples have been trained \\r\" % (batch_ix), 70000 // self.batch_size)\n",
    "                sys.stdout.flush()\n",
    "                # Just test\n",
    "                '''if D_global_step % 50 == 0:\n",
    "                    print(D_global_step, train_D_loss, train_G_loss)\n",
    "                if D_global_step > 5001:\n",
    "                    real_score, fake_score = sess.run([self._D_real, self._D_fake], \n",
    "                                      feed_dict={\n",
    "                                          self.noise: noise,\n",
    "                                          self.real_image:X_batch,\n",
    "                                          self.is_training: False\n",
    "                                      })\n",
    "                    print(real_score[:10])\n",
    "                    print(fake_score[:10])\n",
    "                    print('=' * 80)\n",
    "                    preview = sess.run(self.fake_image, \n",
    "                                       feed_dict={\n",
    "                                           self.noise: noise,\n",
    "                                           self.is_training: False\n",
    "                                       })\n",
    "                    return preview'''\n",
    "        print(\"Training finished!\")    \n",
    "    def generate(self, num):\n",
    "        sess = self.sess\n",
    "        noise = np.random.uniform(-1, 1, [num, config.latent_size])\n",
    "        fake_image = sess.run(self.fake_image,\n",
    "                             feed_dict={\n",
    "                                 self.noise: noise,\n",
    "                                 is_training: False\n",
    "                             })\n",
    "        return fake_image.reshape(num, 28, 28)\n",
    "    def load_mnist_data(self, path=\"mnist.npz\"):\n",
    "        data = np.load(path)\n",
    "        X = np.concatenate((data[\"x_train\"], data[\"x_test\"]), axis=0) / 255.0\n",
    "        y = np.concatenate((data[\"y_train\"], data[\"y_test\"]), axis=0) / 255.0\n",
    "        X = np.expand_dims(X, axis=3)\n",
    "        return X, y\n",
    "    \n",
    "    def minibatches(self, inputs=None, targets=None, batch_size=None, shuffle=False):\n",
    "        assert len(inputs) == len(targets)\n",
    "        if shuffle:\n",
    "            indices = np.arange(len(inputs))\n",
    "            np.random.shuffle(indices)\n",
    "        for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
    "            if shuffle:\n",
    "                excerpt = indices[start_idx:start_idx + batch_size]\n",
    "            else:\n",
    "                excerpt = slice(start_idx, start_idx + batch_size)\n",
    "            yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Starting training...\n",
      "Epoch 1\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "config = Config(batch_size=64, latent_size=100, lr=0.0002, epoch_num=1, beta1=0.5, alpha=0.2)\n",
    "test_input = tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.float32)\n",
    "test_model = DCGAN(test_input, tf.Session(), config)\n",
    "test_model.train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preview = test_model.generate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16825827898>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1682569f588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preview[10:11].reshape(28, 28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
