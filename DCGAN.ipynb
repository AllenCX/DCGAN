{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys \n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self, batch_size, latent_size, lr, beta1, epoch_num, alpha):\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_size = latent_size\n",
    "        self.epoch_num = epoch_num\n",
    "        self.alpha = alpha\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        #self.is_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "    \n",
    "    def __init__(self, input, sess, config):\n",
    "        self.input = input\n",
    "        '''self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_size = latent_size'''\n",
    "        self.X, self.y = self.load_mnist_data()\n",
    "        self.config = config\n",
    "        self.sess = sess\n",
    "        self.idx = 0\n",
    "    def _leakyRelu(self, input, alpha, name='leakyRelu'):\n",
    "        return tf.maximum(input, alpha * input, name=name)\n",
    "        \n",
    "    def _D_conv2d_bn(self, input, output_num, filter_size, stride, is_training=False, name=None):\n",
    "        with tf.variable_scope(name) as scope:\n",
    "            conv = tf.contrib.layers.conv2d(input, output_num, filter_size, stride, \n",
    "                                            'SAME', activation_fn=None, biases_initializer=None) \n",
    "            bn = tf.contrib.layers.batch_norm(inputs=conv, is_training=is_training, activation_fn=None)\n",
    "            return self._leakyRelu(bn, self.config.alpha)\n",
    "        \n",
    "    def _G_conv_transpose_bn(self, input, output_num, filter_size, stride, is_training, name=None):\n",
    "        with tf.variable_scope(name) as scope:\n",
    "            conv_transpose = tf.contrib.layers.conv2d_transpose(\n",
    "                input, output_num, filter_size, stride, 'SAME', \n",
    "                activation_fn=None, biases_initializer=None, scope=name)\n",
    "            bn = tf.contrib.layers.batch_norm(inputs=conv_transpose,\n",
    "                                              is_training=is_training, \n",
    "                                              activation_fn=None, \n",
    "                                              scope=name)\n",
    "            return tf.nn.relu(bn)\n",
    "        \n",
    "    def discriminator(self, input, y=None, reuse=False, is_training=False):\n",
    "        # LeNet-like discriminator\n",
    "        with tf.variable_scope(\"discriminator\") as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "            d_1 = self._D_conv2d_bn(input, 8, 3, 1, is_training, name=\"D_conv1\") # => [None, 14, 14, 8]\n",
    "            d_2 = self._D_conv2d_bn(d_1, 16, 3, 2, is_training, name=\"D_conv2\")  # => [None, 7, 7, 16]\n",
    "            d_3 = self._D_conv2d_bn(d_2, 32, 3, 2, is_training, name=\"D_conv3\")  # => [None, 7, 7, 16]\n",
    "            d_4 = self._D_conv2d_bn(d_3, 64, 3, 1, is_training, name=\"D_conv4\")  # => [None, 7, 7, 16]\n",
    "            d_5 = tf.contrib.layers.flatten(d_4)\n",
    "            d_6 = tf.contrib.layers.fully_connected(d_5, num_outputs=1, activation_fn=None, scope=\"D_linear\")\n",
    "            return tf.nn.sigmoid(d_6, name=\"D_output\"), d_6\n",
    "        \n",
    "    def generator(self, input, reuse=False, is_training=False):\n",
    "        with tf.variable_scope(\"generator\") as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variable()\n",
    "            g_0 = tf.cast(\n",
    "                tf.contrib.layers.fully_connected(input, 7 * 7 * 128, \n",
    "                                                  activation_fn=None, \n",
    "                                                  scope=\"G_linear\"), \n",
    "                dtype=tf.float32)\n",
    "            # => [None, 4 * 4 * 1024]\n",
    "            g_1 = tf.cast(tf.reshape(g_0, shape=[-1, 7, 7, 128]), dtype=tf.float32)\n",
    "            g_2 = self._G_conv_transpose_bn(g_1, 64, 3, 2, is_training=is_training, name=\"G_conv_trans_1\")\n",
    "            g_3 = self._G_conv_transpose_bn(g_2, 32, 3, 1, is_training=is_training, name=\"G_conv_trans_2\")\n",
    "            g_4 = self._G_conv_transpose_bn(g_3, 16, 3, 2, is_training=is_training, name=\"G_conv_trans_3\")\n",
    "            g_5 = self._G_conv_transpose_bn(g_4, 1, 3, 1, is_training=is_training, name=\"G_conv_trans_4\")\n",
    "            return tf.nn.tanh(g_5)\n",
    "            \n",
    "    def build(self, config):\n",
    "        self.is_training = tf.placeholder(bool, name=\"is_training\")\n",
    "        self.noise = tf.placeholder(shape=(config.batch_size, config.latent_size),\n",
    "                                    dtype=tf.float32, name=\"noise\")\n",
    "        self.real_image = tf.placeholder(shape=(config.batch_size, 28, 28, 1), \n",
    "                                         dtype=tf.float32, name=\"real_image\")\n",
    "        \n",
    "        self.fake_image = self.generator(self.noise, is_training=self.is_training)\n",
    "        self._D_fake, self.fake_logits = self.discriminator(self.fake_image, \n",
    "                                                            y=None,\n",
    "                                                            is_training=self.is_training, \n",
    "                                                            reuse=False)\n",
    "        self._D_real, self.real_logits = self.discriminator(self.real_image,\n",
    "                                                            y=None, \n",
    "                                                            is_training=self.is_training, \n",
    "                                                            reuse=True)\n",
    "        \n",
    "        \n",
    "        self.G_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_logits, \n",
    "                                                    labels=tf.ones_like(self._D_fake)))\n",
    "        self.D_loss_fake = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_logits, \n",
    "                                                    labels=tf.zeros_like(self._D_fake)))\n",
    "        \n",
    "        self.D_loss_real = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.real_logits, \n",
    "                                                    labels=tf.ones_like(self._D_real)))\n",
    "        self.D_loss = self.D_loss_real + self.D_loss_fake\n",
    "        \n",
    "        self.G_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=\"generator\") \n",
    "        self.D_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=\"discriminator\")\n",
    "        self.updata_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        \n",
    "        self.G_trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"generator\")\n",
    "        self.D_trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"discriminator\")\n",
    "        self.trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        \n",
    "    def train(self, config):\n",
    "        with tf.control_dependencies(self.D_update_ops):\n",
    "            D_opt = tf.train.AdamOptimizer(learning_rate=config.lr, beta1=config.beta1) \\\n",
    "                            .minimize(self.D_loss, var_list=self.D_trainable_vars)\n",
    "        with tf.control_dependencies(self.G_update_ops):\n",
    "            G_opt = tf.train.AdamOptimizer(learning_rate=config.lr, beta1=config.beta1) \\\n",
    "                            .minimize(self.G_loss, var_list=self.D_trainable_vars)\n",
    "        \n",
    "        noise = np.random.uniform(-1, 1, [config.batch_size, config.latent_size])\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        #for epoch in range(config.epoch_num):\n",
    "        #    x_batch, y_batch = next_batch()\n",
    "    def evaluate():\n",
    "        pass\n",
    "    def test():\n",
    "        pass\n",
    "    def load_mnist_data(self, path=\"mnist.npz\"):\n",
    "        data = np.load(path)\n",
    "        X = np.concatenate((data[\"x_train\"], data[\"x_test\"]), axis=0)\n",
    "        y = np.concatenate((data[\"y_test\"], data[\"y_test\"]), axis=0)\n",
    "        X = np.expand_dims(X, axis=3)\n",
    "        return X, y\n",
    "    \n",
    "    def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False):\n",
    "        assert len(inputs) == len(targets)\n",
    "        if shuffle:\n",
    "            indices = np.arange(len(inputs))\n",
    "            np.random.shuffle(indices)\n",
    "        for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
    "            if shuffle:\n",
    "                excerpt = indices[start_idx:start_idx + batch_size]\n",
    "            else:\n",
    "                excerpt = slice(start_idx, start_idx + batch_size)\n",
    "            yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = Config(batch_size=1, latent_size=100, lr=0.0002, epoch_num=1, beta1=0.5, alpha=0.2)\n",
    "sess = tf.Session()\n",
    "test_input = tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.float32)\n",
    "\n",
    "test_model = DCGAN(test_input, tf.Session(), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_model.build(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_model.train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XX = np.arange(10)\n",
    "y  = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 9]\n"
     ]
    }
   ],
   "source": [
    "for a, b in test_model.next_batch(XX, y, 2, False):\n",
    "    print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = np.load(\"mnist.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x_test', 'x_train', 'y_train', 'y_test']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['x_test'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt = np.concatenate((dataset['x_train'], dataset['x_test']), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [0 1]\n",
      "[2 3] [2 3]\n",
      "[4 5] [4 5]\n",
      "[6 7] [6 7]\n",
      "[8 9] [8 9]\n"
     ]
    }
   ],
   "source": [
    "for i,j in minibatches(XX, y, 2):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
